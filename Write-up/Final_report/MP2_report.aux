\relax 
\citation{kirk2012bayesian}
\citation{kirk2012bayesian}
\citation{andrieu2010particle}
\citation{griffin2014sequential}
\citation{griffin2014sequential}
\citation{kirk2012bayesian}
\citation{kirk2012bayesian}
\citation{griffin2014sequential}
\citation{griffin2014sequential}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}}
\citation{jain1988algorithms}
\citation{pena1999empirical}
\citation{pena1999empirical}
\citation{fraley2002model}
\citation{fraley2002model}
\@writefile{toc}{\contentsline {section}{\numberline {2}Background}{2}}
\newlabel{sec:background}{{2}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Hierarchical cluster analysis}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}K-means}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Finite Mixture Model}{2}}
\citation{escobar1995bayesian}
\citation{ahmad2007k}
\citation{ahmad2007k}
\citation{mcparland2014clustering}
\citation{mcparland2014clustering}
\citation{mcparland2016clustering}
\citation{kirk2012bayesian}
\citation{kirk2012bayesian}
\citation{andrieu2010particle}
\citation{griffin2014sequential}
\citation{griffin2014sequential}
\citation{griffin2014sequential}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4}Nonparametric Mixture Model}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5}Clustering with multiple data sources}{3}}
\citation{kirk2012bayesian}
\citation{fritsch2009improved}
\citation{fritsch2009improved}
\citation{binder1978bayesian}
\citation{wade2015bayesian}
\citation{wade2015bayesian}
\citation{kirk2012bayesian}
\citation{kirk2012bayesian}
\citation{kirk2012bayesian}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6}Sequential Monte Carlo clustering methods}{4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.7}Point estimates from Bayesian clustering models}{4}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Methods}{4}}
\newlabel{sec:methods}{{3}{4}}
\citation{kirk2012bayesian}
\citation{griffin2014sequential}
\citation{griffin2014sequential}
\citation{griffin2014sequential}
\citation{kirk2012bayesian}
\citation{kirk2012bayesian}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Graphical representation of the model in the case where $K = 3$., as described in \citep  {kirk2012bayesian} for the independent case (left) and the MDI model (right). $x_{i,k}$ represents the $i^{th}$ observation in dataset k, which is generated by mixture component $c_{i,k}$. The prior probabilities associated with the component allocation variables $[c_{1, k}, \dots  , c_{N, k}]$ are assigned a prior probability $\pi _k$, which is itself assigned a symmetric Dirichlet prior with parameter $\alpha /N$. The parameter $\theta _{c, k}$ is assigned a $G^0_k$ prior. In the case of the MDI model, the $\phi _{l, k}$ parameter models the dependence between cluster allocations in dataset $l$ and dataset $k$. This is in contrast to the independent case, where there is no information shared between the data sources.\relax }}{5}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{tik:alg}{{1}{5}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces Gibbs sampler\relax }}{5}}
\newlabel{alg:gibbs}{{1}{5}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {2}{\ignorespaces Particle filter to update cluster allocations\relax }}{6}}
\newlabel{alg:pf}{{2}{6}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Example application}{6}}
\newlabel{sec:application}{{4}{6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Data}{6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Results}{6}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.1}Computational cost}{6}}
\citation{mason2016mdi}
\citation{mason2016mdi}
\citation{mdipp}
\citation{savage2010discovering}
\citation{savage2010discovering}
\citation{kirk2012bayesian}
\citation{kirk2012bayesian}
\citation{griffin2014sequential}
\citation{griffin2014sequential}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.2}Allocation accuracy}{7}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.3}Markov chain diagnostics}{7}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Conclusions and future work}{7}}
\newlabel{sec:conclusions}{{5}{7}}
\citation{griffin2014sequential}
\citation{griffin2014sequential}
\bibstyle{plainnat}
\bibdata{bibliography.bib}
\bibcite{ahmad2007k}{{1}{2007}{{Ahmad and Dey}}{{}}}
\bibcite{andrieu2010particle}{{2}{2010}{{Andrieu et~al.}}{{Andrieu, Doucet, and Holenstein}}}
\bibcite{binder1978bayesian}{{3}{1978}{{Binder}}{{}}}
\bibcite{escobar1995bayesian}{{4}{1995}{{Escobar and West}}{{}}}
\bibcite{fraley2002model}{{5}{2002}{{Fraley and Raftery}}{{}}}
\bibcite{fritsch2009improved}{{6}{2009}{{Fritsch et~al.}}{{Fritsch, Ickstadt, et~al.}}}
\bibcite{griffin2014sequential}{{7}{2014}{{Griffin}}{{}}}
\bibcite{jain1988algorithms}{{8}{1988}{{Jain and Dubes}}{{}}}
\bibcite{kirk2012bayesian}{{9}{2012}{{Kirk et~al.}}{{Kirk, Griffin, Savage, Ghahramani, and Wild}}}
\bibcite{mdipp}{{10}{2014}{{Mason}}{{}}}
\bibcite{mason2016mdi}{{11}{2016}{{Mason et~al.}}{{Mason, Sayyid, Kirk, Starr, and Wild}}}
\bibcite{mcparland2014clustering}{{12}{2014}{{McParland et~al.}}{{McParland, Gormley, McCormick, Clark, Kabudula, and Collinson}}}
\bibcite{mcparland2016clustering}{{13}{2016}{{McParland et~al.}}{{McParland, Phillips, Brennan, Roche, and Gormley}}}
\bibcite{pena1999empirical}{{14}{1999}{{Pena et~al.}}{{Pena, Lozano, and Larranaga}}}
\bibcite{savage2010discovering}{{15}{2010}{{Savage et~al.}}{{Savage, Ghahramani, Griffin, Bernard, and Wild}}}
\bibcite{wade2015bayesian}{{16}{2015}{{Wade and Ghahramani}}{{}}}
\citation{griffin2014sequential}
\citation{griffin2014sequential}
\citation{mdipp}
\citation{mdipp}
\citation{mdipp}
\citation{mdipp}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces The results of running the pMDI algorithm (top) and the MDI algorithm (bottom) on four example datasets: two gaussian datasets and two multinomial datasets. The four panels down the right represent the clustering allocation suggested for each dataset individually. With observations running along both axes, the darkness of a square indicates the frequency with which two clusters are assigned to the same allocation across iterations of the algorithm. The ticks on the axes indicate the inferred cluster partition. The main panel represents the consensus agreement across datasets. The allocations inferred in each plot are identical despite the different orderings of the observations.\relax }}{11}}
\newlabel{fig:consensus}{{2}{11}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces One feature from the simulated data used to test the MDI algorithm. The benefit of sharing clustering information across datasets can be seen by considering clusters 1 \& 2 in \texttt  {Gaussian Test Data 1}: given the overlap, it would be difficult to distinguish between the clusters without the added information gained from the additional datasets.\relax }}{12}}
\newlabel{fig:data}{{3}{12}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces The pairwise `fusion' across datasets in the pMDI algorithm (top) as compared with the original MDI (bottom). Lines indicate the cumulative fusion of individual genes in the two datasets. Code to generate plots sourced from \cite  {mdipp}. \relax }}{13}}
\newlabel{fig:allocation}{{4}{13}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Posterior mass associated with the number of fused genes across datasets in pMDI (top) and the original MDI (bottom). The x-axis represents the number of fused genes, the y-axis is the relative frequency. The dashed line represents the mean number of fused genes. Code to generate plots sourced from \cite  {mdipp}.\relax }}{14}}
\newlabel{fig:hist}{{5}{14}}
